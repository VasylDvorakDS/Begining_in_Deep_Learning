{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya6YMjdPW0Lr"
      },
      "source": [
        "# Задание\n",
        "\n",
        "Будем работать с моделью BERT.\n",
        "Цель задания — познакомиться с моделью на примере задачи Questions\n",
        "Answering (ответы на вопросы). Для этого необходимо:\n",
        "1. До установить нужные пакеты\n",
        "2. Cоздать файл q&a.py в коде которого.\n",
        "3. Загрузить пред обученную модель Bert\n",
        "4. Задать контекст — это текст в котором модель будет искать ответы на те вопросы, которые  ей задают\n",
        "5. Реализовать бота, который будет принимать строку ( вопрос) и\n",
        "передавать его модели, а затем выведет на экран ответ.\n",
        "Для создания такого бота с использованием BERT, мы будем использовать\n",
        "библиотеку transformers от Hugging Face и framework flask для создания\n",
        "веб-приложения. Этот бот будет обрабатывать\n",
        "пользовательские запросы и отвечать на них с использованием модели BERT.\n",
        "Конкретная BERT модель выбирается с учетом типа задачи, в данном случае\n",
        "нужно использовать BertForQuestionAnswering, внутри у нее необходимая\n",
        "архитектура, при этом голова\n",
        "соответствует задаче ответов на вопросы.\n",
        "После запуска кода выведет адрес web страницы, перейдя по нему,\n",
        "должны увидеть web интерфейс.\n",
        "В качестве контекста (параграфа) используем кусочек из Алисы в стране чудес,\n",
        "он на английском, так как модель училась на этом языке.\n",
        "Для проверки можно использовать следующие вопросы:\n",
        "1. \"Who did Alice follow down the rabbit hole?\"\n",
        "2. \"What did Alice see as she was falling down the well?\"\n",
        "3. \"How did Alice manage to shrink in size?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "227t7eVveJMU"
      },
      "outputs": [],
      "source": [
        "# Установка нужных библиотек:\n",
        "# transformers — для загрузки предобученной модели BERT,\n",
        "# gradio — для создания простого веб-интерфейса.\n",
        "!pip install -q transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKOZOarKeMk0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Импорт библиотек:\n",
        "import torch  # Библиотека для работы с нейросетями и тензорами (массивами)\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering  # Загрузка токенизатора и модели BERT\n",
        "import gradio as gr  # Простая библиотека для создания интерфейсов\n",
        "import urllib.request  # Чтобы скачать текст книги с сайта\n",
        "# Импортирует функцию truncate из модуля os.\n",
        "# Эта функция используется для обрезки (усечения) файла до заданного размера (в байтах).\n",
        "from os import truncate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j6tpqlb9eQF9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Определяем, какое устройство использовать:\n",
        "# - Если доступна видеокарта (GPU) — используем её\n",
        "# - Если доступен чип Apple (MPS) — используем его\n",
        "# - Иначе используем обычный процессор (CPU)\n",
        "device = (\n",
        "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
        "    torch.device(\"cpu\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ToJcsFpeWzd",
        "outputId": "a1b20133-8173-4094-985f-f927ea0d5104"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Загружаем предобученный токенизатор BERT.\n",
        "# Он преобразует текст в числа (токены), понятные модели.\n",
        "# Модель обучена отвечать на вопросы (SQuAD датасет).\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
        ")\n",
        "\n",
        "# Загружаем саму модель BERT для задачи \"вопрос-ответ\".\n",
        "# Она предсказует, где в тексте начинается и заканчивается ответ.\n",
        "model = BertForQuestionAnswering.from_pretrained(\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
        ").to(device)  # Переносим модель на выбранное устройство (CPU, GPU)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qv7wklPhebs0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Скачиваем текст книги \"Алиса в стране чудес\" с сайта Project Gutenberg\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "urllib.request.urlretrieve(url, \"alice.txt\")\n",
        "\n",
        "# Читаем файл и соединяем все строки в один длинный текст\n",
        "with open('alice.txt', 'r', encoding='utf-8') as file:\n",
        "    full_text = ' '.join(file.readlines())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibiU0fEpeex1",
        "outputId": "f20c13ab-f295-4aed-b43c-1349908ca90a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (37027 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Функция для разбиения текста на части, чтобы не превышать лимит BERT\n",
        "# BERT работает максимум с 512 токенами, поэтому мы используем запас — 400\n",
        "def split_context(text, max_tokens=400):\n",
        "    # Преобразуем весь текст в список токенов (чисел)\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    chunks = []  # список для хранения фрагментов текста\n",
        "    # Разбиваем токены на части по max_tokens\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk = tokens[i:i + max_tokens]  # берём фрагмент\n",
        "        chunks.append(tokenizer.decode(chunk))  # превращаем его обратно в текст в виде списка кусков исходного текста\n",
        "    return chunks  # возвращаем список фрагментов\n",
        "\n",
        "# Разбиваем текст книги один раз заранее\n",
        "context_chunks = split_context(full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7dy2Vvuep5D"
      },
      "outputs": [],
      "source": [
        "# Главная функция для ответа на вопросы\n",
        "def get_answer(question):\n",
        "    best_answer = \"\"  # сюда запишется лучший ответ\n",
        "    # Устанавливаем начальное значение для лучшего найденного \"скор\" (оценки уверенности модели).\n",
        "    # float('-inf') означает \"минус бесконечность\" — гарантирует, что первый настоящий результат будет лучше.\n",
        "    best_score = float('-inf')  # начальное значение — минимально возможное\n",
        "\n",
        "    # Проходим по каждому фрагменту текста\n",
        "    for context in context_chunks:\n",
        "        # Создаём вход для модели: пара \"вопрос + фрагмент текста\"\n",
        "        inputs = tokenizer.encode_plus(\n",
        "            question,       # вопрос\n",
        "            context,        # кусок текста, в котором ищем ответ\n",
        "            add_special_tokens=True,  # добавить служебные токены [CLS], [SEP]\n",
        "            return_tensors='pt',      # вернуть PyTorch-тензоры\n",
        "            truncation=True,          # обрезать, если слишком длинно\n",
        "            max_length=512            # максимальная длина (ограничение BERT)\n",
        "        )\n",
        "\n",
        "        # Перемещаем входные данные на нужное устройство (CPU или GPU)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Пропускаем через модель без обучения (no_grad = без вычисления градиентов)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Модель возвращает два массива:\n",
        "        # start_logits — вероятность начала ответа на каждом токене\n",
        "        # end_logits — вероятность конца ответа на каждом токене\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        # Считаем \"уверенность\" модели как сумму наибольших значений\n",
        "        score = start_logits.max() + end_logits.max()\n",
        "\n",
        "        # Если модель уверена больше, чем раньше — сохраняем этот ответ\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            answer_start = torch.argmax(start_logits)  # индекс начала\n",
        "            answer_end = torch.argmax(end_logits) + 1  # индекс конца (+1 потому что end — включительно)\n",
        "            input_ids = inputs['input_ids'][0]  # сами токены\n",
        "            answer_tokens = input_ids[answer_start:answer_end]  # выделяем токены ответа\n",
        "            best_answer = tokenizer.decode(answer_tokens)  # переводим токены обратно в текст\n",
        "\n",
        "    # Если ответ пустой, вернём \"Ответ не найден\"\n",
        "    return best_answer if best_answer.strip() else \"Ответ не найден.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "XnLy4DeudmZM",
        "outputId": "8c25be50-6c0c-4927-d2e0-9c583869c1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b24cce2290dd527f63.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://b24cce2290dd527f63.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing dataset file at: .gradio/flagged/dataset1.csv\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://095894e630ff71cf2f.gradio.live\n",
            "Killing tunnel 127.0.0.1:7860 <> https://40eb81694743812f3b.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://b24cce2290dd527f63.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Создаём простой веб-интерфейс с помощью Gradio:\n",
        "# - fn=get_answer — функция, которая будет вызываться\n",
        "# - inputs=\"text\" — пользователь вводит текст (вопрос)\n",
        "# - outputs=\"text\" — выводим текст (ответ)\n",
        "iface = gr.Interface(\n",
        "    fn=get_answer,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"BERT Q&A on Alice in Wonderland\",  # заголовок интерфейса\n",
        "    description=\"Введите вопрос на английском языке — я найду ответ в книге 'Alice in Wonderland'\"\n",
        ")\n",
        "\n",
        "# Запуск интерфейса. share=True позволяет получить ссылку на ваш интерфейс\n",
        "iface.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# вывод:\n",
        "\n",
        "В коде реализована полноценная модель \"вопрос–ответ\" на базе BERT, с возможностью задавать вопросы к книге Alice in Wonderland через простой веб-интерфейс на Gradio.\n",
        "\n",
        "В коде реализовано:\n",
        "\n",
        "Загрузка предобученной модели bert-large-uncased (fine-tuned на SQuAD),\n",
        "\n",
        "Разбивка текста книги на части (400 токенов) (для обхода ограничения в 512 токенов),\n",
        "\n",
        "Поиск наиболее вероятного отрывока, содержащего ответ,\n",
        "\n",
        "Отображение результата пользователю через веб-интерфейс gradio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lt6qqZMmeFa9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QjXj4oEAdmWF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "o1WJA_8LdmL8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
